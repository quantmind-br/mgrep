# mgrep Configuration Example
# ============================
# Copy this file to .mgreprc.yaml in your project root
# or to ~/.config/mgrep/config.yaml for global settings.
#
# Configuration precedence (highest to lowest):
# 1. CLI flags
# 2. Environment variables (MGREP_*)
# 3. Local config (.mgreprc.yaml in project directory)
# 4. Global config (~/.config/mgrep/config.yaml)
# 5. Default values

# =============================================================================
# Qdrant Vector Database
# =============================================================================
# mgrep uses Qdrant for local vector storage. Run Qdrant via Docker:
#   docker run -p 6333:6333 qdrant/qdrant

qdrant:
  # Qdrant server URL
  # Env: MGREP_QDRANT_URL
  # Default: http://localhost:6333
  url: http://localhost:6333

  # API key for Qdrant Cloud (optional, not needed for local)
  # Env: MGREP_QDRANT_API_KEY
  # apiKey: your-qdrant-cloud-api-key

  # Prefix for collection names
  # Env: MGREP_QDRANT_COLLECTION_PREFIX
  # Default: mgrep_
  collectionPrefix: mgrep_

# =============================================================================
# Embeddings Provider
# =============================================================================
# Configure the provider for generating text embeddings.
# Supported: openai, google, ollama

embeddings:
  # Provider type
  # Env: MGREP_EMBEDDINGS_PROVIDER
  # Default: openai
  # Options: openai | google | ollama
  provider: openai

  # Model name
  # Env: MGREP_EMBEDDINGS_MODEL
  # Default: text-embedding-3-small
  # Examples:
  #   OpenAI: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002
  #   Google: gemini-embedding-001
  #   Ollama: nomic-embed-text, mxbai-embed-large
  model: text-embedding-3-small

  # Custom base URL (required for Ollama, optional for others)
  # Env: MGREP_EMBEDDINGS_BASE_URL
  # Examples:
  #   Ollama: http://localhost:11434/v1
  #   Azure OpenAI: https://your-resource.openai.azure.com
  # baseUrl: http://localhost:11434/v1

  # API key (optional, falls back to provider-specific env vars)
  # Env: MGREP_EMBEDDINGS_API_KEY
  # Falls back to: OPENAI_API_KEY, GOOGLE_API_KEY
  # apiKey: sk-...

  # Vector dimensions (optional, auto-detected if not set)
  # dimensions: 1536

  # Batch size for embedding requests
  # Default: 100
  batchSize: 100

  # Request timeout in milliseconds
  # Env: MGREP_EMBEDDINGS_TIMEOUT_MS
  # Default: 30000 (30 seconds)
  timeoutMs: 30000

  # Maximum retry attempts for failed requests
  # Env: MGREP_EMBEDDINGS_MAX_RETRIES
  # Default: 3
  maxRetries: 3

# =============================================================================
# LLM Provider (for Question Answering)
# =============================================================================
# Configure the LLM used for the --answer flag (RAG pipeline).
# Supported: openai, google, anthropic, ollama

llm:
  # Provider type
  # Env: MGREP_LLM_PROVIDER
  # Default: openai
  # Options: openai | google | anthropic | ollama
  provider: openai

  # Model name
  # Env: MGREP_LLM_MODEL
  # Default: gpt-4o-mini
  # Examples:
  #   OpenAI: gpt-4o, gpt-4o-mini, gpt-4-turbo
  #   Google: gemini-2.0-flash, gemini-3-flash-preview, gemini-3-pro-preview
  #   Anthropic: claude-3-5-sonnet-20241022, claude-3-haiku-20240307
  #   Ollama: llama3.2, mistral, codellama
  model: gpt-4o-mini

  # Custom base URL (required for Ollama, optional for others)
  # Env: MGREP_LLM_BASE_URL
  # Examples:
  #   Ollama: http://localhost:11434/v1
  #   Azure OpenAI: https://your-resource.openai.azure.com
  # baseUrl: http://localhost:11434/v1

  # API key (optional, falls back to provider-specific env vars)
  # Env: MGREP_LLM_API_KEY
  # Falls back to: OPENAI_API_KEY, GOOGLE_API_KEY, ANTHROPIC_API_KEY
  # apiKey: sk-...

  # Temperature for response generation (0.0 - 2.0)
  # Env: MGREP_LLM_TEMPERATURE
  # Default: 0.7
  temperature: 0.7

  # Maximum tokens in response
  # Env: MGREP_LLM_MAX_TOKENS
  # Default: 4096
  maxTokens: 4096

  # Request timeout in milliseconds
  # Env: MGREP_LLM_TIMEOUT_MS
  # Default: 60000 (60 seconds)
  timeoutMs: 60000

  # Maximum retry attempts for failed requests
  # Env: MGREP_LLM_MAX_RETRIES
  # Default: 3
  maxRetries: 3

# =============================================================================
# Sync Settings
# =============================================================================
# Configure file synchronization behavior.

sync:
  # Number of concurrent file uploads
  # Env: MGREP_SYNC_CONCURRENCY
  # Default: 20
  # Note: Lower this for rate-limited APIs or slow connections
  concurrency: 20

# =============================================================================
# General Settings
# =============================================================================

# Maximum file size in bytes (files larger are skipped)
# Env: MGREP_MAX_FILE_SIZE
# Default: 10485760 (10 MB)
maxFileSize: 10485760

# =============================================================================
# Example Configurations
# =============================================================================

# --- Fully Local with Ollama ---
# embeddings:
#   provider: ollama
#   model: nomic-embed-text
#   baseUrl: http://localhost:11434/v1
# llm:
#   provider: ollama
#   model: llama3.2
#   baseUrl: http://localhost:11434/v1

# --- OpenAI (default) ---
# embeddings:
#   provider: openai
#   model: text-embedding-3-small
# llm:
#   provider: openai
#   model: gpt-4o-mini

# --- Google Gemini ---
# embeddings:
#   provider: google
#   model: gemini-embedding-001
# llm:
#   provider: google
#   model: gemini-3-flash-preview

# --- Mixed: OpenAI embeddings + Anthropic LLM ---
# embeddings:
#   provider: openai
#   model: text-embedding-3-small
# llm:
#   provider: anthropic
#   model: claude-3-5-sonnet-20241022
